{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Lyric Generation Dropout Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurriol2/dgd_lyric_generation/blob/ft-rnn/Lyric_Generation_Dropout_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UYkMyY9gwH8",
        "colab_type": "text"
      },
      "source": [
        "# Lyric Generation with a Recurrent Neural Network\n",
        "## Experimenting with different model topologies ##  \n",
        "\n",
        "---\n",
        "# Dropout Topology #\n",
        "\n",
        "This is a continuation of the previous notebook. Here, I focus on finding ways to improve the model by changing the model architecture.\n",
        "\n",
        "The base model demonstrates the overall ML workflow for training a RNN. The focus for the remaining parts of this notebook will be improving the predictive power of the RNN.\n",
        "\n",
        "LSTMs can easily overfit the data. Adding dropout layers to the model topology can reduce the amount of overfitting.\n",
        "\n",
        "Dropout layers can have different *dropout rates*. Adding dropout layers also adds the dropout layer as a tunable hyperparameter.\n",
        "\n",
        "**Question** `LSTM` layer has a dropout parameter, but I'm not sure if it works the same as a `Dropout` layer\n",
        "\n",
        "**Idea** It might be helpful to define layer building functions. Then layers are added from a list of initialized layers. This approach might make grid search for hyperparameters easier to automate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVImJlNhgwH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNY6zftfgwIA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "21ffc19d-5295-4db2-be87-4dacf30dc550"
      },
      "source": [
        "#a single .csv containing Dance Gavin Dance lyrics \n",
        "filepath = \"https://raw.githubusercontent.com/nurriol2/dgd_lyric_generation/ft-rnn/dance_gavin_dance_lyrics.txt\"\n",
        "text = requests.get(filepath).text\n",
        "#print the first few characters to check that this is the data we expect \n",
        "print(text[:250])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Verse 1: Tilian & Jon Mess]\n",
            "Do you crave a greater reason to exist?\n",
            "Have you always known that symmetry is bliss?\n",
            "We know you see the pattern\n",
            "Lay in your lap, think of your path\n",
            "Philosophy don't bother me, come back when you're trash\n",
            "You are welcome\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwfGP9mHKkmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalization step by reducing the vocabulary size\n",
        "text = text.lower()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0veKTPMgwIF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb4380be-f088-42c1-bfe7-09671a8edbab"
      },
      "source": [
        "#total number of characters in the file\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 257869 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LDPkY92gwII",
        "colab_type": "text"
      },
      "source": [
        "*vocabulary* - set of all elements that make up the sequence data \n",
        "- elements in this case are characters\n",
        "- characters are unique:  A != a \n",
        "- needs to be converted to an ingestible form for the model (aka numbers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NaTGh36gwII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f83d57fa-e7cf-4e31-80e8-ac932ba4fc1c"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcfnYZbqgwIL",
        "colab_type": "text"
      },
      "source": [
        "**Note 1**  \n",
        "Apostrophes and commas are currently part of the vocabulary. The model might predict that the next character is a comma when (as a human) it would make more sense to predict the letter \"m\". So, a simple improvement might be removing such characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGx3LfHpgwIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoding characters as integers\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "#a decode map to get text as output, instead of integers\n",
        "idx2char = np.array(vocab)\n",
        "#vectorize the text\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a6rZmV7gwIP",
        "colab_type": "text"
      },
      "source": [
        "## Overview of the problem workflow ##\n",
        "- The model is fed *a sequence* with a specific length *n*\n",
        "- The model tries to predict the next *most probable* character, **based on the last n characters**\n",
        "\n",
        "## What the X_ and y_ look like ##\n",
        "Pretend the sequence length *n*==4. Then, an (input, output) pair might look like this  \n",
        "(\"Hell\", \"ello\")  \n",
        "\n",
        "The process of making a training-testing dataset with this format is ~~automated by the function~~ begins with\n",
        "`tf.data.Dataset.from_tensor_slices`. The data is sliced along axis=0 to create a new `Dataset` obj"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZjFbMW_gwIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "64e48a73-3559-4214-8470-f82478ce9ebc"
      },
      "source": [
        "#the maximum length sentence we want for a single input in characters\n",
        "seq_length = 87\n",
        "#the quotient here makes sense because there can only be \"quotient\" number of sequences in the text\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "### create (training examples, targets)###\n",
        "\n",
        "#from_tensor_slices -> slice along axis=0\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "#a Dataset with 5 elements\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])\n",
        "\n",
        "#combine consecutive elements from Dataset obj into another Dataset\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\n",
            "v\n",
            "e\n",
            "r\n",
            "s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDl44KVtgwIS",
        "colab_type": "text"
      },
      "source": [
        "## What's going on with `drop_remiander=True`? ##\n",
        "There is no gurantee that the quotient (len of dataset)/(seq len) is an integer. In the case that the last batch is smaller than the desired sequence length, this param lets you drop/include the batch.  \n",
        "\n",
        "It might be interesting to check this quotient directly and see if (in the case of this data) the last batch is being dropped and if the model might perform better including the extra examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaG23y9UgwIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    \"\"\"\n",
        "    Form the input and target by shifting a fixed length window 1 character forward\n",
        "    \n",
        "    Args:\n",
        "    chunk (str):  The input sequence\n",
        "    \n",
        "    Returns:\n",
        "    (tuple):  A pair of strings, (input text, target text)\n",
        "    \"\"\"\n",
        "    \n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "#apply this function to all sequences\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6sY_V7sgwIX",
        "colab_type": "text"
      },
      "source": [
        "At this point, all we've done is create a labeled dataset that can be used to train the model.  \n",
        "\n",
        "Upcoming printed text is human-readable example of what we want the model to do. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb6nDgfDgwIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2877ce40-e17f-4d3d-ba68-c9770359cdb2"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  '[verse 1: tilian & jon mess]\\ndo you crave a greater reason to exist?\\nhave you always kn'\n",
            "Target data: 'verse 1: tilian & jon mess]\\ndo you crave a greater reason to exist?\\nhave you always kno'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMjufMB9gwIa",
        "colab_type": "text"
      },
      "source": [
        "*From TensorFlow Tutorial* - Understanding text as a time series\n",
        "\n",
        ">Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, **it does the same thing but the RNN considers the previous step context in addition to the current input character**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGF_6QssgwIa",
        "colab_type": "text"
      },
      "source": [
        "## Shuffling and splitting time series data ##  \n",
        "\n",
        "**Shuffling** - Shuffling in this context has to be viewed differently than other sequential data. I don't actually care that the model learns patterns from \"The Jiggler\" before learning from \"Prisoner\". I would expect relatively the same performance from any order because **the well ordered temporal axis is NOT the order of the songs**. Instead, **the temporal axis is the order of the characters in each sequence**.  \n",
        "\n",
        "*Key Point*: Shuffling the order of each sequence **produces an equivalent representation of the dataset** - all of the songs are still there! In contrast to a sequential dataset where the temporal axis is actually time (historical stock prices) shuffling those sequences **produces a fundamentally different dataset**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFa9N7m7gwIb",
        "colab_type": "text"
      },
      "source": [
        "**Note 2** Part of the tuning phase is balancing the number of epochs and batch size. (As of right now, this is a heuristic) $\\Rightarrow$ Increasing epochs and reducing batch size will give the model more opportunity to be updated and learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBLeCcK2gwIb",
        "colab_type": "text"
      },
      "source": [
        "# A different way of reshaping the input\n",
        "\n",
        "We know that the model expects data in with this shape: `(number of samples, sequence length, number of featuers)`.    \n",
        "\n",
        "In the block below, the data is being reshaped into groups of `(number of samples, sequence length)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEZ3Gy-KgwIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64bfcd34-ddc3-483f-9cb3-dcee53035edd"
      },
      "source": [
        "#the number of examples to propogate\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((50, 87), (50, 87)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lgzp95TgwIe",
        "colab_type": "text"
      },
      "source": [
        "# Building the model #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAfrCBn0gwIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of training samples\n",
        "nsamples = len(text)\n",
        "\n",
        "#length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "#the input layer size\n",
        "embedding_dim = 256\n",
        "\n",
        "#number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jgC6ijBgwIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "\n",
        "def build_dropout_model(vocab_size, embedding_dim, rnn_units, batch_size, dropout_rate=0.24):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size,\n",
        "                        embedding_dim,\n",
        "                        batch_input_shape=[batch_size, None]))\n",
        "    model.add(LSTM(rnn_units,\n",
        "                   return_sequences=True,\n",
        "                   stateful=True,\n",
        "                   recurrent_initializer=\"glorot_uniform\"))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(vocab_size))    \n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_noz5nygwIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dropout model\n",
        "model = build_dropout_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRrambmVgwIn",
        "colab_type": "text"
      },
      "source": [
        "## Output Shape ##\n",
        "\n",
        "Only passing in vectors of (batch, sequence length) and yet outputting (batch, sequence length, vocab length). The indices of the last dimension reflects the probability that the i-th character is predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z4HOR-7gwIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a861689c-1ec5-45bc-df83-91e5a723c351"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(\"EXAMPLE INPUT: {}\".format(input_example_batch.shape))\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXAMPLE INPUT: (50, 87)\n",
            "(50, 87, 63) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbCeXnIdgwIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fe9277e1-d9ea-40e7-bffa-15bca3828d59"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (50, None, 256)           16128     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (50, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (50, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (50, None, 63)            64575     \n",
            "=================================================================\n",
            "Total params: 5,327,679\n",
            "Trainable params: 5,327,679\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2YuTRJHgwIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "36d336f2-163f-44cf-c1f1-609f5c913c1e"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14, 58, 31, 20,  3, 52,  3,  6, 54, 50, 40, 56, 26, 33, 27, 57, 15,\n",
              "       52,  9, 38, 39, 23, 26, 52, 52, 14, 29, 43, 13,  8, 51, 55, 17,  8,\n",
              "        8, 13,  0, 10, 45,  4,  7,  0, 38, 12, 47, 30, 61,  0, 18,  2,  9,\n",
              "       29, 51, 18, 18, 50, 11, 26, 57, 33,  5,  7, 22, 62, 39,  3, 19, 49,\n",
              "       21, 22, 59, 59, 44, 14, 41, 59, 57, 19, 44,  7, 13, 26, 27,  6, 43,\n",
              "       54, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wby-mB93gwI1",
        "colab_type": "text"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5q7K04HgwI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7cAcehvgwI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ016zQMgwI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFVj_ReCogoN",
        "colab_type": "text"
      },
      "source": [
        "# Using Best Weights for Prediction #\n",
        "\n",
        "ModelCheckpoint has a `save_best_only` option that works with the `monitor` parameter. This option is how we are going to ensure that predictions are made with weights that minimize the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjzFJYobgwI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43068ac3-8536-4fb7-fd09-979f573f68c0"
      },
      "source": [
        "EPOCHS=39\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/39\n",
            "58/58 [==============================] - 3s 53ms/step - loss: 3.0832\n",
            "Epoch 2/39\n",
            "58/58 [==============================] - 3s 52ms/step - loss: 2.3823\n",
            "Epoch 3/39\n",
            "58/58 [==============================] - 3s 53ms/step - loss: 2.0268\n",
            "Epoch 4/39\n",
            "58/58 [==============================] - 3s 53ms/step - loss: 1.8360\n",
            "Epoch 5/39\n",
            "58/58 [==============================] - 3s 53ms/step - loss: 1.6955\n",
            "Epoch 6/39\n",
            "58/58 [==============================] - 3s 53ms/step - loss: 1.5876\n",
            "Epoch 7/39\n",
            "58/58 [==============================] - 3s 53ms/step - loss: 1.4961\n",
            "Epoch 8/39\n",
            "58/58 [==============================] - 3s 53ms/step - loss: 1.4088\n",
            "Epoch 9/39\n",
            "58/58 [==============================] - 3s 54ms/step - loss: 1.3264\n",
            "Epoch 10/39\n",
            "58/58 [==============================] - 3s 54ms/step - loss: 1.2387\n",
            "Epoch 11/39\n",
            "58/58 [==============================] - 3s 54ms/step - loss: 1.1554\n",
            "Epoch 12/39\n",
            "58/58 [==============================] - 3s 54ms/step - loss: 1.0695\n",
            "Epoch 13/39\n",
            "58/58 [==============================] - 3s 54ms/step - loss: 0.9819\n",
            "Epoch 14/39\n",
            "58/58 [==============================] - 3s 54ms/step - loss: 0.9025\n",
            "Epoch 15/39\n",
            "58/58 [==============================] - 3s 55ms/step - loss: 0.8267\n",
            "Epoch 16/39\n",
            "58/58 [==============================] - 3s 55ms/step - loss: 0.7561\n",
            "Epoch 17/39\n",
            "58/58 [==============================] - 3s 55ms/step - loss: 0.6906\n",
            "Epoch 18/39\n",
            "58/58 [==============================] - 3s 55ms/step - loss: 0.6378\n",
            "Epoch 19/39\n",
            "58/58 [==============================] - 3s 55ms/step - loss: 0.5904\n",
            "Epoch 20/39\n",
            "58/58 [==============================] - 3s 55ms/step - loss: 0.5514\n",
            "Epoch 21/39\n",
            "58/58 [==============================] - 3s 56ms/step - loss: 0.5136\n",
            "Epoch 22/39\n",
            "58/58 [==============================] - 3s 56ms/step - loss: 0.4847\n",
            "Epoch 23/39\n",
            "58/58 [==============================] - 3s 56ms/step - loss: 0.4595\n",
            "Epoch 24/39\n",
            "58/58 [==============================] - 3s 56ms/step - loss: 0.4363\n",
            "Epoch 25/39\n",
            "58/58 [==============================] - 3s 57ms/step - loss: 0.4179\n",
            "Epoch 26/39\n",
            "58/58 [==============================] - 3s 56ms/step - loss: 0.4030\n",
            "Epoch 27/39\n",
            "58/58 [==============================] - 3s 57ms/step - loss: 0.3872\n",
            "Epoch 28/39\n",
            "58/58 [==============================] - 3s 58ms/step - loss: 0.3747\n",
            "Epoch 29/39\n",
            "58/58 [==============================] - 3s 57ms/step - loss: 0.3628\n",
            "Epoch 30/39\n",
            "58/58 [==============================] - 3s 58ms/step - loss: 0.3541\n",
            "Epoch 31/39\n",
            "58/58 [==============================] - 3s 58ms/step - loss: 0.3442\n",
            "Epoch 32/39\n",
            "58/58 [==============================] - 3s 58ms/step - loss: 0.3389\n",
            "Epoch 33/39\n",
            "58/58 [==============================] - 3s 59ms/step - loss: 0.3320\n",
            "Epoch 34/39\n",
            "58/58 [==============================] - 3s 59ms/step - loss: 0.3232\n",
            "Epoch 35/39\n",
            "58/58 [==============================] - 3s 60ms/step - loss: 0.3188\n",
            "Epoch 36/39\n",
            "58/58 [==============================] - 3s 60ms/step - loss: 0.3123\n",
            "Epoch 37/39\n",
            "58/58 [==============================] - 4s 60ms/step - loss: 0.3049\n",
            "Epoch 38/39\n",
            "58/58 [==============================] - 4s 62ms/step - loss: 0.3016\n",
            "Epoch 39/39\n",
            "58/58 [==============================] - 4s 61ms/step - loss: 0.2977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjyNkYgGgwJA",
        "colab_type": "text"
      },
      "source": [
        "**Question**\n",
        "When saving checkpoints (in the block below), where is the gurantee that calling the weights with the minimum loss? Does this block just assume that SGD landed at the minimum? \n",
        "\n",
        "**Note 3** Might be better to include the loss in the name of the checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Mfy17SgwJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "29c25858-e2e3-4af4-90ce-2c4895ecfa33"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_39'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kanjos2XgwJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_dropout_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y35RFPA8gwJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "beb25e31-d056-404c-e02f-b44832df064d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16128     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (1, None, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 63)             64575     \n",
            "=================================================================\n",
            "Total params: 5,327,679\n",
            "Trainable params: 5,327,679\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aKwC6aCgwJL",
        "colab_type": "text"
      },
      "source": [
        "# Prediction Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7ugb2hygwJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    \"\"\"\n",
        "    Generate text using a trained model\n",
        "    \n",
        "    Args:\n",
        "    model (tensorflow.keras.Model):  A trained model\n",
        "    start_string (str):  The starting input sequence\n",
        "    \n",
        "    Returns:\n",
        "    (str):  The predicted text. Concatenation of start_string and following `num_generate` predicted characters.\n",
        "    \"\"\"\n",
        "\n",
        "    #number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    #converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    #storing the predicted indices (wrt look-up table)\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 0.97\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L9mvLR2gwJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "52fb3a58-b22b-4921-b650-6450413061b8"
      },
      "source": [
        "print(generate_text(model, start_string=u\"[intro: \"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[intro: tilian & jon mess]\n",
            "now)\n",
            "i don't just sit around counting all my money\n",
            "this is how you go out\n",
            "i don't just sit around counting and move me with the feel of lie\n",
            "\n",
            "[break: jon mess]\n",
            "that's my prode my world\n",
            "don't let it go, we won't heap\n",
            "\n",
            "[chorus: pilian]\n",
            "i want to know the truth\n",
            "well, you can ask me a question\n",
            "i'll tell you s eetles rat trect\n",
            "tunned it all\n",
            "\n",
            "[pre-chorus: tilian]\n",
            "can't let it go\n",
            "what hurts, i didn't wait for could ous ore in the changer)\n",
            "i know we talked about the end of the storach on a midal a manic\n",
            "(chorus: tilian]\n",
            "don't delete me\n",
            "\n",
            "[chorus 3: kurt travis]\n",
            "(please, girl, stay..)\n",
            "and i'm not cravill and but oh)\n",
            "'cuse to be at alare\n",
            "so i'm of swople through the dnest to see, what i believe\n",
            "it's all that\n",
            "fall in line with me, it's all that i see, what i believe\n",
            "it's all that\n",
            "fall in line with me, it's all that i see, what i believe\n",
            "it's all that\n",
            "fall in line with me, it's all that i see, what i believed, i'm 'bout to make a new slang\n",
            "if you. wouldn’t deal my take a houp\n",
            "\n",
            "[ch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRUHhjBCLtMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"dgd_lyric_gen.h5\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8qjaZFU7-jv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdc355c5-7975-4588-8716-30af5ddfd8f2"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dgd_lyric_gen.h5  sample_data  training_checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYd4noZU8Ctw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "bb34ec4e-f48b-4442-fa41-1c9999df574b"
      },
      "source": [
        "new_model = tf.keras.models.load_model(\"dgd_lyric_gen.h5\")\n",
        "new_model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16128     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (1, None, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 63)             64575     \n",
            "=================================================================\n",
            "Total params: 5,327,679\n",
            "Trainable params: 5,327,679\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54iojzsJ8Dmp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "53ea3c41-9ea5-451d-d63d-a2cb9a2f5f3f"
      },
      "source": [
        "print(generate_text(new_model, start_string=u\"[intro: \"))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[intro: tilian]\n",
            "sick of being on my game gown\n",
            "why don't you quit?\n",
            "you'll never get better, you'll never get over it\n",
            "why don't you quit? you'll be fine\n",
            "purge, then binge, restart and in my own mind in my soul\n",
            "i know that they ain't never gonna break me\n",
            "(damaged pride and vulnearable)\n",
            "i think the tree is a thief\n",
            "i'm fry the best tongs)\n",
            "how we're all under attack from everything always\n",
            "(paid out and spread the second time\n",
            "'cause i know you'll always be waiting to settle the score\n",
            "here comes the reaper to erase when i'm 'bout to make and save your less &nd splinting me again\n",
            "\n",
            "we're all skin?\n",
            "don't believe the rume\n",
            "i we really fucked it all up this time\n",
            "we got so low (only for at the sign\n",
            "i can see it and it's auding to so (just tried to keep all these bitch that gives a shit, bottles it\n",
            "deliberately swallows it, one less contestant life\n",
            "they'll provide tho doda time is ore t take a kit again\n",
            "\n",
            "[chorus: tilian]\n",
            "don't wanna be a shoulder to cry, well, i grew mess]\n",
            "and i no, the liasted this sy head\n",
            "s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VHsLpwwFcS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef6f717c-26b8-4b43-91ea-3e42db0a2312"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp42Czb1IaJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da8e0971-7a7d-4c50-a274-601e9123ca4d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dgd_lyric_gen.h5  sample_data  training_checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUEAbVAcIbZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SUJ3sBXIe1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}