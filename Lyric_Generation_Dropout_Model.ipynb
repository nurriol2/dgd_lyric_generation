{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Lyric Generation Dropout Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UYkMyY9gwH8",
        "colab_type": "text"
      },
      "source": [
        "# Lyric Generation with a Recurrent Neural Network\n",
        "## Experimenting with different model topologies ##  \n",
        "\n",
        "---\n",
        "# Dropout Topology #\n",
        "\n",
        "This is a continuation of the previous notebook. Here, I focus on finding ways to improve the model by changing the model architecture.\n",
        "\n",
        "The base model demonstrates the overall ML workflow for training a RNN. The focus for the remaining parts of this notebook will be improving the predictive power of the RNN.\n",
        "\n",
        "LSTMs can easily overfit the data. Adding dropout layers to the model topology can reduce the amount of overfitting.\n",
        "\n",
        "Dropout layers can have different *dropout rates*. Adding dropout layers also adds the dropout layer as a tunable hyperparameter.\n",
        "\n",
        "**Question** `LSTM` layer has a dropout parameter, but I'm not sure if it works the same as a `Dropout` layer\n",
        "\n",
        "**Idea** It might be helpful to define layer building functions. Then layers are added from a list of initialized layers. This approach might make grid search for hyperparameters easier to automate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVImJlNhgwH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNY6zftfgwIA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2e13334f-767c-4ec9-e31c-55b7540c7d35"
      },
      "source": [
        "#a single .csv containing Dance Gavin Dance lyrics \n",
        "filepath = \"https://raw.githubusercontent.com/nurriol2/dgd_lyric_generation/ft-rnn/dance_gavin_dance_lyrics.txt\"\n",
        "text = requests.get(filepath).text\n",
        "#print the first few characters to check that this is the data we expect \n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Verse 1: Tilian & Jon Mess]\n",
            "Do you crave a greater reason to exist?\n",
            "Have you always known that symmetry is bliss?\n",
            "We know you see the pattern\n",
            "Lay in your lap, think of your path\n",
            "Philosophy don't bother me, come back when you're trash\n",
            "You are welcome\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0veKTPMgwIF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4dbff0c1-72f7-4b59-8745-2e784ee8f9b4"
      },
      "source": [
        "#total number of characters in the file\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 257869 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LDPkY92gwII",
        "colab_type": "text"
      },
      "source": [
        "*vocabulary* - set of all elements that make up the sequence data \n",
        "- elements in this case are characters\n",
        "- characters are unique:  A != a \n",
        "- needs to be converted to an ingestible form for the model (aka numbers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NaTGh36gwII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9464b1c8-cd75-4d50-a4a7-3f8084f58a05"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "89 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcfnYZbqgwIL",
        "colab_type": "text"
      },
      "source": [
        "**Note 1**  \n",
        "Apostrophes and commas are currently part of the vocabulary. The model might predict that the next character is a comma when (as a human) it would make more sense to predict the letter \"m\". So, a simple improvement might be removing such characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGx3LfHpgwIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoding characters as integers\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "#a decode map to get text as output, instead of integers\n",
        "idx2char = np.array(vocab)\n",
        "#vectorize the text\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a6rZmV7gwIP",
        "colab_type": "text"
      },
      "source": [
        "## Overview of the problem workflow ##\n",
        "- The model is fed *a sequence* with a specific length *n*\n",
        "- The model tries to predict the next *most probable* character, **based on the last n characters**\n",
        "\n",
        "## What the X_ and y_ look like ##\n",
        "Pretend the sequence length *n*==4. Then, an (input, output) pair might look like this  \n",
        "(\"Hell\", \"ello\")  \n",
        "\n",
        "The process of making a training-testing dataset with this format is ~~automated by the function~~ begins with\n",
        "`tf.data.Dataset.from_tensor_slices`. The data is sliced along axis=0 to create a new `Dataset` obj"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZjFbMW_gwIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3a676e72-9102-4820-fa3f-fa599407284e"
      },
      "source": [
        "#the maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "#the quotient here makes sense because there can only be \"quotient\" number of sequences in the text\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "### create (training examples, targets)###\n",
        "\n",
        "#from_tensor_slices -> slice along axis=0\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "#a Dataset with 5 elements\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])\n",
        "\n",
        "#combine consecutive elements from Dataset obj into another Dataset\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\n",
            "V\n",
            "e\n",
            "r\n",
            "s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDl44KVtgwIS",
        "colab_type": "text"
      },
      "source": [
        "## What's going on with `drop_remiander=True`? ##\n",
        "There is no gurantee that the quotient (len of dataset)/(seq len) is an integer. In the case that the last batch is smaller than the desired sequence length, this param lets you drop/include the batch.  \n",
        "\n",
        "It might be interesting to check this quotient directly and see if (in the case of this data) the last batch is being dropped and if the model might perform better including the extra examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaG23y9UgwIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    \"\"\"\n",
        "    Form the input and target by shifting a fixed length window 1 character forward\n",
        "    \n",
        "    Args:\n",
        "    chunk (str):  The input sequence\n",
        "    \n",
        "    Returns:\n",
        "    (tuple):  A pair of strings, (input text, target text)\n",
        "    \"\"\"\n",
        "    \n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "#apply this function to all sequences\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6sY_V7sgwIX",
        "colab_type": "text"
      },
      "source": [
        "At this point, all we've done is create a labeled dataset that can be used to train the model.  \n",
        "\n",
        "Upcoming printed text is human-readable example of what we want the model to do. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb6nDgfDgwIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5296eb47-7c89-41e0-e047-1acc8e32a9fa"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  '[Verse 1: Tilian & Jon Mess]\\nDo you crave a greater reason to exist?\\nHave you always known that symm'\n",
            "Target data: 'Verse 1: Tilian & Jon Mess]\\nDo you crave a greater reason to exist?\\nHave you always known that symme'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMjufMB9gwIa",
        "colab_type": "text"
      },
      "source": [
        "*From TensorFlow Tutorial* - Understanding text as a time series\n",
        "\n",
        ">Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, **it does the same thing but the RNN considers the previous step context in addition to the current input character**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGF_6QssgwIa",
        "colab_type": "text"
      },
      "source": [
        "## Shuffling and splitting time series data ##  \n",
        "\n",
        "**Shuffling** - Shuffling in this context has to be viewed differently than other sequential data. I don't actually care that the model learns patterns from \"The Jiggler\" before learning from \"Prisoner\". I would expect relatively the same performance from any order because **the well ordered temporal axis is NOT the order of the songs**. Instead, **the temporal axis is the order of the characters in each sequence**.  \n",
        "\n",
        "*Key Point*: Shuffling the order of each sequence **produces an equivalent representation of the dataset** - all of the songs are still there! In contrast to a sequential dataset where the temporal axis is actually time (historical stock prices) shuffling those sequences **produces a fundamentally different dataset**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFa9N7m7gwIb",
        "colab_type": "text"
      },
      "source": [
        "**Note 2** Part of the tuning phase is balancing the number of epochs and batch size. (As of right now, this is a heuristic) $\\Rightarrow$ Increasing epochs and reducing batch size will give the model more opportunity to be updated and learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBLeCcK2gwIb",
        "colab_type": "text"
      },
      "source": [
        "# A different way of reshaping the input\n",
        "\n",
        "We know that the model expects data in with this shape: `(number of samples, sequence length, number of featuers)`.    \n",
        "\n",
        "In the block below, the data is being reshaped into groups of `(number of samples, sequence length)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEZ3Gy-KgwIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61370eea-dd15-4b7b-f60b-2141e4ebebb0"
      },
      "source": [
        "#the number of examples to propogate\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lgzp95TgwIe",
        "colab_type": "text"
      },
      "source": [
        "# Building the model #"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAfrCBn0gwIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of training samples\n",
        "nsamples = len(text)\n",
        "\n",
        "#length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "#the input layer size\n",
        "embedding_dim = 256\n",
        "\n",
        "#number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jgC6ijBgwIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "\n",
        "def build_dropout_model(vocab_size, embedding_dim, rnn_units, batch_size, dropout_rate=0.2):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size,\n",
        "                        embedding_dim,\n",
        "                        batch_input_shape=[batch_size, None]))\n",
        "    model.add(LSTM(rnn_units,\n",
        "                   return_sequences=True,\n",
        "                   stateful=True,\n",
        "                   recurrent_initializer=\"glorot_uniform\"))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(vocab_size))    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_noz5nygwIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dropout model\n",
        "model = build_dropout_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRrambmVgwIn",
        "colab_type": "text"
      },
      "source": [
        "## Output Shape ##\n",
        "\n",
        "Only passing in vectors of (batch, sequence length) and yet outputting (batch, sequence length, vocab length). The indices of the last dimension reflects the probability that the i-th character is predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z4HOR-7gwIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b5cb016b-3c11-473d-882e-47d7e1897835"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(\"EXAMPLE INPUT: {}\".format(input_example_batch.shape))\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXAMPLE INPUT: (64, 100)\n",
            "(64, 100, 89) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbCeXnIdgwIq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f315ec66-a7f4-4ad6-d691-73d778b780f6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           22784     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (64, None, 1024)          0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 89)            91225     \n",
            "=================================================================\n",
            "Total params: 5,360,985\n",
            "Trainable params: 5,360,985\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2YuTRJHgwIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2685e514-e84e-4fab-fb21-528bc2f2ff69"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([50, 29, 83, 16, 23, 29, 80, 21, 15, 17, 78, 81, 57, 77, 73, 50, 83,\n",
              "       79,  9, 63, 35,  6, 74, 84, 33, 78, 50, 40, 80, 12, 68, 44, 15, 51,\n",
              "       80, 10, 16, 84, 48, 29, 86, 41, 39, 73, 14, 85,  2, 53, 35, 32, 18,\n",
              "       62, 16,  9,  6, 71, 86, 52, 52, 54, 23, 41, 41, 68, 32,  3, 64, 53,\n",
              "       53, 16, 34,  2, 76,  2, 51, 73,  3, 50, 16, 52, 34, 35, 84, 76, 47,\n",
              "       18, 83, 51, 13, 64, 40, 51, 24, 62, 16, 43, 42, 81, 67, 80])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wby-mB93gwI1",
        "colab_type": "text"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5q7K04HgwI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7cAcehvgwI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ016zQMgwI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFVj_ReCogoN",
        "colab_type": "text"
      },
      "source": [
        "# Using Best Weights for Prediction #\n",
        "\n",
        "ModelCheckpoint has a `save_best_only` option that works with the `monitor` parameter. This option is how we are going to ensure that predictions are made with weights that minimize the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjzFJYobgwI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e936db1-52d7-4ee9-ca30-085798f7703f"
      },
      "source": [
        "EPOCHS=40\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 3.3153\n",
            "Epoch 2/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 2.6865\n",
            "Epoch 3/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 2.2263\n",
            "Epoch 4/40\n",
            "39/39 [==============================] - 3s 75ms/step - loss: 2.0042\n",
            "Epoch 5/40\n",
            "39/39 [==============================] - 3s 76ms/step - loss: 1.8563\n",
            "Epoch 6/40\n",
            "39/39 [==============================] - 3s 77ms/step - loss: 1.7465\n",
            "Epoch 7/40\n",
            "39/39 [==============================] - 3s 77ms/step - loss: 1.6550\n",
            "Epoch 8/40\n",
            "39/39 [==============================] - 3s 77ms/step - loss: 1.5765\n",
            "Epoch 9/40\n",
            "39/39 [==============================] - 3s 77ms/step - loss: 1.4994\n",
            "Epoch 10/40\n",
            "39/39 [==============================] - 3s 77ms/step - loss: 1.4296\n",
            "Epoch 11/40\n",
            "39/39 [==============================] - 3s 75ms/step - loss: 1.3579\n",
            "Epoch 12/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 1.2866\n",
            "Epoch 13/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 1.2131\n",
            "Epoch 14/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 1.1341\n",
            "Epoch 15/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 1.0561\n",
            "Epoch 16/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.9800\n",
            "Epoch 17/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.9052\n",
            "Epoch 18/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.8371\n",
            "Epoch 19/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.7693\n",
            "Epoch 20/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.7062\n",
            "Epoch 21/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.6503\n",
            "Epoch 22/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.6054\n",
            "Epoch 23/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.5618\n",
            "Epoch 24/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.5257\n",
            "Epoch 25/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.4934\n",
            "Epoch 26/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.4645\n",
            "Epoch 27/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.4384\n",
            "Epoch 28/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.4176\n",
            "Epoch 29/40\n",
            "39/39 [==============================] - 3s 73ms/step - loss: 0.3994\n",
            "Epoch 30/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.3832\n",
            "Epoch 31/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.3676\n",
            "Epoch 32/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.3552\n",
            "Epoch 33/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.3475\n",
            "Epoch 34/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.3338\n",
            "Epoch 35/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.3251\n",
            "Epoch 36/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.3153\n",
            "Epoch 37/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.3102\n",
            "Epoch 38/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.3035\n",
            "Epoch 39/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.2949\n",
            "Epoch 40/40\n",
            "39/39 [==============================] - 3s 74ms/step - loss: 0.2906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjyNkYgGgwJA",
        "colab_type": "text"
      },
      "source": [
        "**Question**\n",
        "When saving checkpoints (in the block below), where is the gurantee that calling the weights with the minimum loss? Does this block just assume that SGD landed at the minimum? \n",
        "\n",
        "**Note 3** Might be better to include the loss in the name of the checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Mfy17SgwJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dbfd92a6-5ade-4987-c3b4-7fd9c97f5ccc"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_40'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kanjos2XgwJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_dropout_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y35RFPA8gwJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "eae68f64-1d8d-4f4f-de38-64b195b88f75"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            22784     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (1, None, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 89)             91225     \n",
            "=================================================================\n",
            "Total params: 5,360,985\n",
            "Trainable params: 5,360,985\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aKwC6aCgwJL",
        "colab_type": "text"
      },
      "source": [
        "# Prediction Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7ugb2hygwJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    \"\"\"\n",
        "    Generate text using a trained model\n",
        "    \n",
        "    Args:\n",
        "    model (tensorflow.keras.Model):  A trained model\n",
        "    start_string (str):  The starting input sequence\n",
        "    \n",
        "    Returns:\n",
        "    (str):  The predicted text. Concatenation of start_string and following `num_generate` predicted characters.\n",
        "    \"\"\"\n",
        "\n",
        "    #number of characters to generate\n",
        "    num_generate = 1000\n",
        "\n",
        "    #converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    #storing the predicted indices (wrt look-up table)\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L9mvLR2gwJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "8d73f7b7-2362-49ff-bf48-de55e79ceabd"
      },
      "source": [
        "print(generate_text(model, start_string=u\"[Intro: \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Intro: Jon Mess]\n",
            "But the stakes are higher now\n",
            "I'm all treaming backs thrust\n",
            "I could see it gl, well, I believe will go there\n",
            "Give me substang, I can still shear\n",
            "Nowing and nated (Light, and Jon Mess]\n",
            "It's so funny when you what you have to use\n",
            "(Right now I'd be the-)\n",
            "Oh, prestry to give it up\n",
            "Nom, we'll go down together (Go down together)\n",
            "We'll go down together\n",
            "Don't depend on me\n",
            "A let-down forever\n",
            "Stop defending me\n",
            "We'll go down together\n",
            "Don't depend on me\n",
            "A let-down forever\n",
            "Stop defending me\n",
            "We'll go on (It's all these : Kurt Travis], all a rittle word\n",
            "But I don’t care fee\n",
            "I'll be the same tonight\n",
            "I ad the streets I live good you got me never ther be so gidow, then I head\n",
            "Moke it never better style\n",
            "Don't tell you I stolld there\n",
            "I need to make this break when you're driving me in my life\n",
            "Then I'm walling 'couse a long on my mind\n",
            "I'm underw towards me, pacu an allwayd them\n",
            "I feel the second time up with my limes and use dad and Sun\n",
            "Creak me\n",
            "I have something is very we drunk\n",
            "\n",
            "[Intro: Jon Mess\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}