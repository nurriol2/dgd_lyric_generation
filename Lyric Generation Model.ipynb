{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyric Generation with Recurrent Nerual Network #\n",
    "## Generating Dance Gavin Dance lyrics with machine learning ##\n",
    "\n",
    "The starting point for this project is the official [TensorFlow Tutorial - Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation). I made several modifications to the base tutorial to meet my project goals. \n",
    "\n",
    "### Modifications ###\n",
    "The first change has to do with the data itself. Since I am interested in generating song lyrics (instead of Shakespearean sonnets) I'm using a dataset of Dance Gavin Dance song lyrics that I scraped from the web. The web scraping program I wrote and the data itself, are [on my project repo](https://github.com/nurriol2/dgd_lyric_generation). \n",
    "\n",
    "Additionally, the supporting text in this notebook highlights what I think is important *for my own understanding*. At this time, this project is not a tutorial for implementing RNNs (it's more of a project diary).\n",
    "\n",
    "While the tutorial provides some guidance on improving the model, I found great ideas in [Jason Brownlee's post on text generation](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/). I implemented a few of these suggestions throughout this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "I intend for this noteboook **to run on [Google Colab](https://colab.research.google.com/)**.  \n",
    "\n",
    "Here are several resasons I chose Google Colab:\n",
    "1. Even with only a few epochs, my laptop cannot train a model quickly\n",
    "2. Google Colab does not require any installations (Python or otherwise) to get started programming\n",
    "3. The errors I had running the same code with AWS and Spell, simply, do not exist when using Google Colab  \n",
    "\n",
    "If you have a powerful GPU or would like to try running this code on your own machine or somewhere besides Google Colab, everything is available on the project repo - including a `requirements.txt` file. If you do use a different platform to explore this project, I would love to hear about it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verse 1: Tilian & Jon Mess]\n",
      "Do you crave a greater reason to exist?\n",
      "Have you always known that symmetry is bliss?\n",
      "We know you see the pattern\n",
      "Lay in your lap, think of your path\n",
      "Philosophy don't bother me, come back when you're trash\n",
      "You are welcome\n"
     ]
    }
   ],
   "source": [
    "#a single .csv containing Dance Gavin Dance lyrics \n",
    "filepath = \"https://raw.githubusercontent.com/nurriol2/dgd_lyric_generation/ft-rnn/dance_gavin_dance_lyrics.txt\"\n",
    "text = requests.get(filepath).text\n",
    "#print the first few characters to check that this is the data we expect \n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 257869 characters\n"
     ]
    }
   ],
   "source": [
    "#total number of characters in the file\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*vocabulary* - set of all elements that make up the sequence data \n",
    "- elements in this case are characters\n",
    "- characters are unique:  A != a \n",
    "- needs to be converted to an ingestible form for the model (aka numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 1**  \n",
    "Apostrophes and commas are currently part of the vocabulary. The model might predict that the next character is a comma when (as a human) it would make more sense to predict the letter \"m\". So, a simple improvement might be removing such characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verse 1:\n",
      "[51 46 57 70 71 57  1 13 22]\n",
      "V - 46\n",
      ": - 22\n"
     ]
    }
   ],
   "source": [
    "#encoding characters as integers\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "#a decode map to get text as output, instead of integers\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(text[:9])\n",
    "print(text_as_int[:9])\n",
    "\n",
    "#V maps to 51; : maps to 22\n",
    "print(\"V - {}\\n: - {}\".format(char2idx[\"V\"], char2idx[\":\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the problem workflow ##\n",
    "- The model is fed *a sequence* with a specific length *n*\n",
    "- The model tries to predict the next *most probable* character, **based on the last n characters**\n",
    "\n",
    "## What the X_ and y_ look like ##\n",
    "Pretend the sequence length *n*==4. Then, an (input, output) pair might look like this  \n",
    "(\"Hell\", \"ello\")  \n",
    "\n",
    "The process of making a training-testing dataset with this format is ~~automated by the function~~ begins with\n",
    "`tf.data.Dataset.from_tensor_slices`. The data is sliced along axis=0 to create a new `Dataset` obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "V\n",
      "e\n",
      "r\n",
      "s\n",
      "'[Verse 1: Tilian & Jon Mess]\\nDo you crave a greater reason to exist?\\nHave you always known that symme'\n",
      "\"try is bliss?\\nWe\\u2005know\\u2005you see the\\u2005pattern\\nLay in your lap, think of\\u2005your path\\nPhilosophy don't bother\"\n",
      "\" me, come back when you're trash\\nYou are welcome here but you must come alone\\nYou know everything is \"\n",
      "'everywhere is home\\nDo you see it?\\n\\n[Chorus: Tilian]\\nPrisoner, prisoner\\nWe found you\\nWe feel you breat'\n",
      "\"hing\\nAre you there?\\nCan you hear us calling you?\\nWe'll never judge you\\n\\n[Verse 2: Jon Mess & Tillian]\"\n"
     ]
    }
   ],
   "source": [
    "#the maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "#the quotient here makes sense because there can only be \"quotient\" number of sequences in the text\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "### create (training examples, targets)###\n",
    "\n",
    "#from_tensor_slices -> slice along axis=0\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "#a Dataset with 5 elements\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])\n",
    "\n",
    "#combine consecutive elements from Dataset obj into another Dataset\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's going on with `drop_remiander=True`? ##\n",
    "There is no gurantee that the quotient (len of dataset)/(seq len) is an integer. In the case that the last batch is smaller than the desired sequence length, this param lets you drop/include the batch.  \n",
    "\n",
    "It might be interesting to check this quotient directly and see if (in the case of this data) the last batch is being dropped and if the model might perform better including the extra examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    \"\"\"\n",
    "    Form the input and target by shifting a fixed length window 1 character forward\n",
    "    \n",
    "    Args:\n",
    "    chunk (str):  The input sequence\n",
    "    \n",
    "    Returns:\n",
    "    (tuple):  A pair of strings, (input text, target text)\n",
    "    \"\"\"\n",
    "    \n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "\n",
    "    return input_text, target_text\n",
    "\n",
    "#apply this function to all sequences\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all we've done is create a labeled dataset that can be used to train the model.  \n",
    "\n",
    "Upcoming printed text is human-readable example of what we want the model to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '[Verse 1: Tilian & Jon Mess]\\nDo you crave a greater reason to exist?\\nHave you always known that symm'\n",
      "Target data: 'Verse 1: Tilian & Jon Mess]\\nDo you crave a greater reason to exist?\\nHave you always known that symme'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From TensorFlow Tutorial* - Understanding text as a time series\n",
    "\n",
    ">Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, **it does the same thing but the RNN considers the previous step context in addition to the current input character**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling and splitting time series data ##  \n",
    "\n",
    "**Shuffling** - Shuffling in this context has to be viewed differently than other sequential data. I don't actually care that the model learns patterns from \"The Jiggler\" before learning from \"Prisoner\". I would expect relatively the same performance from any order because **the well ordered temporal axis is NOT the order of the songs**. Instead, **the temporal axis is the order of the characters in each sequence**.  \n",
    "\n",
    "*Key Point*: Shuffling the order of each sequence **produces an equivalent representation of the dataset** - all of the songs are still there! In contrast to a sequential dataset where the temporal axis is actually time (historical stock prices) shuffling those sequences **produces a fundamentally different dataset**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 2** Part of the tuning phase is balancing the number of epochs and batch size. (As of right now, this is a heuristic) $\\Rightarrow$ Increasing epochs and reducing batch size will give the model more opportunity to be updated and learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A different way of reshaping the input\n",
    "\n",
    "We know that the model expects data in with this shape: `(number of samples, sequence length, number of featuers)`.    \n",
    "\n",
    "In the block below, the data is being reshaped into groups of `(number of samples, sequence length)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the number of examples to propogate\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "#the input layer size\n",
    "embedding_dim = 256\n",
    "\n",
    "#number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    \"\"\"\n",
    "    Build a Sequential model with specific topology by specifying neuron numbers.\n",
    "    Topology:  Input - LSTM - Dense\n",
    "    Notes:  Follows TF Tutorial exactly. Refer to this model as the \"basic model\" in notes. \n",
    "    \n",
    "    Args:\n",
    "    vocab_size (int):  The number of unique elements that comprise a dataset\n",
    "    embedding_dim (int):  Dimension of the mapping between characters and a dense vector\n",
    "    batch_size (int): The number of memory units (GRU or LSTM)\n",
    "    \n",
    "    Returns:\n",
    "    (tensorflow.keras.Model):  A linear stack of neuron layers\n",
    "    \"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic model\n",
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Shape ##\n",
    "\n",
    "Only passing in vectors of (batch, sequence length) and yet outputting (batch, sequence length, vocab length). The indices of the last dimension reflects the probability that the i-th character is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE INPUT: (64, 100)\n",
      "(64, 100, 89) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(\"EXAMPLE INPUT: {}\".format(input_example_batch.shape))\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           22784     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 89)            91225     \n",
      "=================================================================\n",
      "Total params: 5,360,985\n",
      "Trainable params: 5,360,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([76, 49, 14, 21, 51, 79,  5, 74, 30,  0, 29, 72, 18, 81, 59, 77, 30,\n",
       "       11, 60,  3, 50, 35, 25, 46, 34, 70, 29, 37, 35, 49, 32, 65, 43, 68,\n",
       "       43, 75, 88, 16, 45, 60, 68, 53, 78, 75, 77, 81, 83, 82, 72,  6,  7,\n",
       "       55, 88,  5, 48, 46, 13, 84, 57, 26, 55, 38, 12, 71, 65, 16, 64,  7,\n",
       "       36, 66, 69, 12, 43, 78, 55, 81, 84, 40, 35, 51, 18, 81, 84, 54, 44,\n",
       "        0, 49,  0,  6, 86, 79, 10,  5, 60, 19, 67, 80, 87, 83, 26])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#makes sense b/c it's a predicted sequence\n",
    "len(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"can't keep my mind open now\\nOh-oh, whoa-oh-oh\\n\\n[Verse 2: Tilian]\\nMake up your mind, we're running ou\"\n",
      "\n",
      "Next Char Predictions: \n",
      " 'xY29[ç\\'vF\\nEt6ígyF.h\"ZKAVJrEMKYHmSpSw\\u205f4Uhpazwyí\\u2005út()c\\u205f\\'XV1\\u200aeBcN0sm4l)Lnq0Szcí\\u200aPK[6í\\u200abT\\nY\\n(‚ç-\\'h7oé“\\u2005B'\n"
     ]
    }
   ],
   "source": [
    "#human readable decoding of the prediction\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 89)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.4883547\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "#name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "#using the save_best_only option\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=60\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question #\n",
    "When saving checkpoints (in the block below), where is the gurantee that calling the weights with the minimum loss? Does this block just assume that SGD landed at the minimum? \n",
    "\n",
    "**Note 3** Might be better to include the loss in the name of the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \"\"\"\n",
    "    Generate text using a trained model\n",
    "    \n",
    "    Args:\n",
    "    model (tensorflow.keras.Model):  A trained model\n",
    "    start_string (str):  The starting input sequence\n",
    "    \n",
    "    Returns:\n",
    "    (str):  The predicted text. Concatenation of start_string and following `num_generate` predicted characters.\n",
    "    \"\"\"\n",
    "\n",
    "    #number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    #converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    #storing the predicted indices (wrt look-up table)\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"[Intro: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
